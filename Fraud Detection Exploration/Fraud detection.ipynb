{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark package \n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#data processing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch.nn as nn\n",
    "\n",
    "#可视化\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于 customer_id 下的做 merchant_id 的 embedding，你会怎么做? (实现代码会加分，不知道写可以描述思想)\n",
    "\n",
    "例子： 通过计算每个merchant_id下所有用户的消费行为，比如所有用户在这个产品上平均消费金额，购买产品总数，用户平均年龄等数值型特征均可作为每款产品的embedding   \n",
    "下面的数据我简单挑选了两个feature: 所有用户在这个产品上平均消费金额，购买产品总数,当拥有更多数据的时候可以增加更多feature, 目前根据这两个值产生metrics便是每个产品与用户的联系（embedding 数据库）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 每个商家的流量（客户数量，消费金额）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_q3 = spark.read.csv(\"data_train.csv\",header=True)\n",
    "\n",
    "# for col in df_q3.columns:\n",
    "#     df_q3 = df_q3.withColumn( col, regexp_replace(col , \"'\", \"\"))\n",
    "    \n",
    "# #给数据分类\n",
    "# str_cols = ['customer_id','age_group','gender','merchant_id','type']\n",
    "# int_cols = ['id','time','zipcode_customer','zipcode_merchant','fraud']\n",
    "# double_cols = [ 'amount']\n",
    "\n",
    "# for col in df_q3.columns:\n",
    "#     if col in int_cols:\n",
    "#         df_q3 = df_q3.withColumn(col,f.col(col).cast(\"integer\"))\n",
    "#     elif col in str_cols:\n",
    "#         df_q3 = df_q3.withColumn(col,f.col(col).cast(\"string\"))\n",
    "#     elif col in double_cols:\n",
    "#         df_q3 = df_q3.withColumn(col,f.col(col).cast(\"double\"))  \n",
    "        \n",
    "        \n",
    "        \n",
    "# df_q3.withColumn(\"用户平均消费金额\",avg(\"amount\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"产品售出总数\",count(\"merchant_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "#     .withColumn(\"产品客户数量\",f.approx_count_distinct(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "#     .withColumn(\"平均购买数量\",f.col(\"产品售出总数\")/f.col(\"产品客户数量\"))\\\n",
    "#     .select(\"merchant_id\",\"用户平均消费金额\",\"平均购买数量\")\\\n",
    "#     .distinct()\\\n",
    "#     .orderBy(desc(\"平均购买数量\"))\\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 对于支付宝反欺诈模型，你觉得哪些变量会比较重要(请考虑交易双方的特征)?\n",
    "\n",
    "> 1. 登陆地理位置比较异常，与平时的不太符合，ip地址,zipcode\n",
    "> 2. 突然出现数额异常大的交易额\n",
    "> 3. 突然出现新的收款方或付款方（之前从未出现过，或者没什么交集）\n",
    "> 1. 最近支付比较频繁，与历史的支付习惯差异较大\n",
    "> 1. 账户比较新\n",
    "> 1. 账户里出现比较异常的行为，比如突然改了头像，改了名字\n",
    "> 1. 与平时的转账行为不太一样\n",
    "> 1. 突然加好友\n",
    "> 1. 之前账号是否有被举报的情况或者被盗用的情况\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 处理数据，根据数据类型做对应的处理\n",
    "* 对于类别数据用dummy处理\n",
    "* 对于数值型,用standardscaler处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    \n",
    "    cat_arry = OneHotEncoder(handle_unknown=\"ignore\").fit_transform(df.select(\"age_group\",\"gender\",\"type\").collect())\\\n",
    "                .toarray().astype(np.float32)\n",
    "    \n",
    "    scale_ary = StandardScaler().fit_transform(\n",
    "    np.array(df.select(\"amount\").collect(),dtype=\"float\").astype(np.float32))\n",
    "    \n",
    "    reg_ary = np.array(df.select(\"用户平均消费金额\",\"平均购买数量\").collect(),dtype=\"float\").astype(np.float32)\n",
    "    final_ary = np.concatenate((cat_arry,scale_ary,reg_ary),axis=1)\n",
    "    \n",
    "    return final_ary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 返回每个预测结果的AP值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(y_vali,y_pred):\n",
    "    print(\"AP值为：\",average_precision_score(y_vali, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "* 训练数据集加一列标为train, 测试数据机加一列标为test,方便之后数据处理的分割\n",
    "* 集合训练集和测试集为df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = spark.read.csv(\"data_train.csv\",header=True).withColumn(\"kind\",lit(\"train\")) \n",
    "df_test_raw = spark.read.csv(\"data_test.csv\",header=True).withColumn(\"fraud\",lit(\"unknown\"))\\\n",
    "                               .withColumn(\"kind\",lit(\"test\")) \n",
    "\n",
    "df_train_label = df_train_raw.select(\"id\",\"fraud\")\\\n",
    "                        .withColumn(\"id\",f.col(\"id\").cast(\"integer\"))\\\n",
    "                        .withColumn(\"fraud\",f.col(\"fraud\").cast(\"integer\")) \n",
    "\n",
    "df = df_train_raw.union(df_test_raw.select(df_train_raw.columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "* 去掉数据中的标点符号\n",
    "* 给数据分类，具体分为：integer, string,double\n",
    "* 看预处理后数据的大致情况，先做最显而易见的处理，比如出现不合理的值\n",
    "* 只有训练集出现了5条金额为负数的记录，视为异常值，排除掉,主表重新命名为main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去标点符号\n",
    "for col in df.columns:\n",
    "    df = df.withColumn( col, regexp_replace(col , \"'\", \"\"))\n",
    "    \n",
    "#给数据分类\n",
    "str_cols = ['customer_id','age_group','gender','merchant_id','type']\n",
    "int_cols = ['id','time','zipcode_customer','zipcode_merchant','fraud']\n",
    "double_cols = [ 'amount']\n",
    "\n",
    "for col in df.columns:\n",
    "    \n",
    "    if col in int_cols:\n",
    "        df = df.withColumn(col,f.col(col).cast(\"integer\"))\n",
    "    elif col in str_cols:\n",
    "        df = df.withColumn(col,f.col(col).cast(\"string\"))\n",
    "    elif col in double_cols:\n",
    "        df = df.withColumn(col,f.col(col).cast(\"double\"))   \n",
    "\n",
    "#看每个feature的大概情况\n",
    "# for col in df.columns:\n",
    "#     df.select(col).describe().show()\n",
    "    \n",
    "main_df = df.filter(f.col(\"amount\")>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                id|\n",
      "+-------+------------------+\n",
      "|  count|            296503|\n",
      "|   mean|148252.87730984172|\n",
      "| stddev| 85594.90396831403|\n",
      "|    min|                 0|\n",
      "|    max|            296507|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|             time|\n",
      "+-------+-----------------+\n",
      "|  count|           296503|\n",
      "|   mean|94.97524476986742|\n",
      "| stddev|50.98429852862592|\n",
      "|    min|                0|\n",
      "|    max|              179|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----------+\n",
      "|summary|customer_id|\n",
      "+-------+-----------+\n",
      "|  count|     296503|\n",
      "|   mean|       null|\n",
      "| stddev|       null|\n",
      "|    min|C1000148617|\n",
      "|    max| C999723254|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|         age_group|\n",
      "+-------+------------------+\n",
      "|  count|            296503|\n",
      "|   mean|3.0249833703069657|\n",
      "| stddev|1.3611465801196447|\n",
      "|    min|                 0|\n",
      "|    max|                 U|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------+\n",
      "|summary|gender|\n",
      "+-------+------+\n",
      "|  count|296503|\n",
      "|   mean|  null|\n",
      "| stddev|  null|\n",
      "|    min|     E|\n",
      "|    max|     U|\n",
      "+-------+------+\n",
      "\n",
      "+-------+----------------+\n",
      "|summary|zipcode_customer|\n",
      "+-------+----------------+\n",
      "|  count|          296503|\n",
      "|   mean|         28007.0|\n",
      "| stddev|             0.0|\n",
      "|    min|           28007|\n",
      "|    max|           28007|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-----------+\n",
      "|summary|merchant_id|\n",
      "+-------+-----------+\n",
      "|  count|     296503|\n",
      "|   mean|       null|\n",
      "| stddev|       null|\n",
      "|    min|M1053599405|\n",
      "|    max| M980657600|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+----------------+\n",
      "|summary|zipcode_merchant|\n",
      "+-------+----------------+\n",
      "|  count|          296503|\n",
      "|   mean|         28007.0|\n",
      "| stddev|             0.0|\n",
      "|    min|           28007|\n",
      "|    max|           28007|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|                type|\n",
      "+-------+--------------------+\n",
      "|  count|              296503|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|es_barsandrestaur...|\n",
      "|    max|es_wellnessandbeauty|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            amount|\n",
      "+-------+------------------+\n",
      "|  count|            296503|\n",
      "|   mean|37.642161023666006|\n",
      "| stddev|111.17744838831037|\n",
      "|    min|               0.0|\n",
      "|    max|           7635.41|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|               fraud|\n",
      "+-------+--------------------+\n",
      "|  count|              296503|\n",
      "|   mean|0.011655868574685585|\n",
      "| stddev| 0.10733148725100632|\n",
      "|    min|                   0|\n",
      "|    max|                   1|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------+\n",
      "|summary|  kind|\n",
      "+-------+------+\n",
      "|  count|296503|\n",
      "|   mean|  null|\n",
      "| stddev|  null|\n",
      "|    min| train|\n",
      "|    max| train|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in main_df.columns:\n",
    "    main_df.filter(f.col(\"kind\")==\"train\").select(a).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析\n",
    "1. 观察得出 train & test dataset年龄层，性别还有交易类型均匀分布\n",
    "2. 了解测试和训练数据中数据分布情况\n",
    "> a. 训练数据里标签非常不平衡，只有1%的数据是fraud  \n",
    "> b. 顾客的区域均为28007，商铺的区域也均为28007  \n",
    "> c. 消费金额平均在37，38上下  \n",
    "> d. 测试和训练数据中消费总流量（包括重复的客户）比较接近，总客户数一致（unique values),总商铺数量一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyze_cols = ['age_group','gender','type']\n",
    "# describe_cols = ['time','zipcode_customer','zipcode_merchant','amount','fraud']\n",
    "# unique_cols = ['customer_id','merchant_id']\n",
    "\n",
    "# #训练和预测数据里年龄性别还有产品交易类型分布均匀\n",
    "# print(\"展示categorical数据分布情况\")\n",
    "# for col in analyze_cols:\n",
    "#     main_df.groupBy(col,'kind').count()\\\n",
    "#     .groupBy(col).pivot('kind').agg(first(\"count\"))\\\n",
    "#     .withColumn(\"cts_dif\",f.col(\"train\")-f.col(\"test\"))\\\n",
    "#     .withColumn(\"dif_pct\",f.round(f.col(\"cts_dif\")/(f.col(\"train\")+f.col(\"test\"))/2 ,2)).show()\n",
    "\n",
    "# print(\"训练数据集情况\")\n",
    "# main_df.filter(f.col(\"kind\")==\"train\").select(describe_cols).describe().show()\n",
    "# print(\"测试数据集情况\")\n",
    "# main_df.filter(f.col(\"kind\")==\"test\").select(describe_cols).describe().show()\n",
    "\n",
    "# print(\"在训练和测试集中顾客总流量（包括重复的顾客）\")\n",
    "# main_df.select(\"customer_id\",\"kind\").groupBy(\"kind\").count().show()\n",
    "# print(\"unique 顾客的数量\")\n",
    "# main_df.select(\"customer_id\",\"kind\").distinct().groupBy(\"kind\").count().show()\n",
    "\n",
    "# print(\"在训练和测试集中商铺总数量（不包括重复的）\")\n",
    "# # main_df.select(\"merchant_id\",\"kind\").groupBy(\"kind\").count().show()\n",
    "# main_df.select(\"merchant_id\",\"kind\").distinct().groupBy(\"kind\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程1: 分割数据为df_train,df_vali,df_test然后做特征处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > 1. 每个商铺的用户平均消费金额\n",
    " > 1. 每个用户在商铺下的平均购买数量\n",
    " > 5. 交易金额\n",
    " > 6. 交易产品类型\n",
    " > 1. 性别\n",
    " > 1. 年龄层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同一个客户一共购买的次数，=》 在这家店购买次数的比例，同一商品种类下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_main = main_df.filter(f.col(\"kind\")==\"train\") \n",
    "df_train, df_vali = df_train_main.randomSplit([0.75, 0.25],seed = 10)\n",
    "\n",
    "embedding1 = df_train.withColumn(\"用户平均消费金额\",avg(\"amount\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    ".withColumn(\"产品售出总数\",count(\"merchant_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "    .withColumn(\"产品客户数量\",f.approx_count_distinct(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "    .withColumn(\"平均购买数量\",f.col(\"产品售出总数\")/f.col(\"产品客户数量\"))\\\n",
    "    .select(\"merchant_id\",\"用户平均消费金额\",\"平均购买数量\")\\\n",
    "    .distinct()\n",
    "\n",
    "df_train = df_train.join(embedding1,\"merchant_id\",\"left\")\\\n",
    "            .drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")\n",
    "            \n",
    "\n",
    "df_vali = df_vali.join(embedding1,\"merchant_id\",\"left\")\\\n",
    "            .drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'time',\n",
       " 'age_group',\n",
       " 'gender',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'fraud',\n",
       " '用户平均消费金额',\n",
       " '平均购买数量']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vali.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_main = main_df.filter(f.col(\"kind\")==\"train\") \n",
    "# df_train, df_vali = df_train_main.randomSplit([0.75, 0.25],seed = 10)\n",
    "\n",
    "# c_m_relation = df_train.withColumn(\"客户流量\",count(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"客户distinct数量\",f.approx_count_distinct(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"用户总消费金额\",sum(\"amount\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"平均消费金额\",f.col(\"用户总消费金额\") / f.col(\"客户distinct数量\"))\\\n",
    "# .select(\"merchant_id\",\"客户流量\",\"平均消费金额\")\\\n",
    "#     .distinct()\n",
    "\n",
    "# df_train = df_train.join(c_m_relation,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")\n",
    "\n",
    "# df_vali = df_vali.join(c_m_relation,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_main = main_df.filter(f.col(\"kind\")==\"train\") \n",
    "# df_train, df_vali = df_train_main.randomSplit([0.75, 0.25],seed = 10)\n",
    "\n",
    "# c_m_relation = df_train.withColumn(\"客户流量\",count(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"客户distinct数量\",f.approx_count_distinct(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"用户总消费金额\",sum(\"amount\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "# .withColumn(\"平均消费金额\",f.col(\"用户总消费金额\") / f.col(\"客户distinct数量\"))\\\n",
    "# .withColumn(\"客户在同一家消费次数\",count(\"customer_id\").over(Window.partitionBy(\"merchant_id\",\"customer_id\")))\\\n",
    "# .select(\"merchant_id\",\"客户流量\",\"平均消费金额\",\"客户在同一家消费次数\")\\\n",
    "#     .distinct()\n",
    "\n",
    "# df_train = df_train.join(c_m_relation,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")\n",
    "\n",
    "# df_vali = df_vali.join(c_m_relation,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\",\"zipcode_customer\",\"zipcode_merchant\",\"kind\")\\\n",
    "# fillna(1, subset=[\"客户在同一家消费次数\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Safe 信息\")\n",
    "# print(df_train.filter(f.col(\"fraud\")== 0).drop(\"id\").describe().toPandas())\n",
    "# print(\"\\n\")\n",
    "# print(\"Fraud 信息\")\n",
    "# print(df_train.filter(f.col(\"fraud\")== 1).drop(\"id\").describe().toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程 2\n",
    "\n",
    "* 将交易类型做dummy处理\n",
    "* 交易金额，时间戳，每家店顾客总流量用standardscaler处理\n",
    "* 其他数据保持不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'time',\n",
       " 'age_group',\n",
       " 'gender',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'fraud',\n",
       " '用户平均消费金额',\n",
       " '平均购买数量']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    \n",
    "    cat_arry = OneHotEncoder(handle_unknown=\"ignore\").fit_transform(df.select(\"age_group\",\"gender\",\"type\").collect())\\\n",
    "                .toarray().astype(np.float32)\n",
    "    \n",
    "    scale_ary = StandardScaler().fit_transform(\n",
    "    np.array(df.select(\"amount\",\"time\",\"用户平均消费金额\",\"平均购买数量\").collect(),dtype=\"float\").astype(np.float32))\n",
    "    \n",
    "#     reg_ary = np.array(df.select(\"amount\",\"time\",\"客户在同一家消费次数\").collect(),dtype=\"float\").astype(np.float32)\n",
    "    final_ary = np.concatenate((cat_arry,scale_ary),axis=1)\n",
    "    \n",
    "    return final_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (222087, 31)\n",
      "y_train shape (222087, 1)\n",
      "X_vali shape (74416, 31)\n",
      "y_vali shape (74416, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = process_data(df_train)\n",
    "y_train = np.array(df_train.select(\"fraud\").collect(),dtype=\"float\").astype(np.float32)\n",
    "\n",
    "X_vali = process_data(df_vali)\n",
    "y_vali = np.array(df_vali.select(\"fraud\").collect(),dtype=\"float\").astype(np.float32)\n",
    "\n",
    "print(\"X_train shape\",X_train.shape)\n",
    "print(\"y_train shape\",y_train.shape)\n",
    "print(\"X_vali shape\",X_vali.shape)\n",
    "print(\"y_vali shape\",y_vali.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isnan(y_train).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isnan(y_vali).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单猜测预测1的可能性\n",
    "p_1 = np.sum(y_train)/(len(y_train))\n",
    "y_base = np.full(y_vali.shape , p_1, dtype=float)\n",
    "\n",
    "result(y_vali,y_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP值为： 0.7893854605187441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_line = regressor.predict(X_vali)\n",
    "result(y_vali,y_pred_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=74416, minmax=(array([-2.1622343], dtype=float32), array([2.7754402], dtype=float32)), mean=array([0.01255421], dtype=float32), variance=array([0.00706812], dtype=float32), skewness=array([10.628092], dtype=float32), kurtosis=array([172.67578], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.describe(y_pred_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP值为： 0.8457885779639692\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_log = classifier.predict_proba(X_vali)\n",
    "result(y_vali,y_pred_log[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=74416, minmax=(1.7915749998505477e-06, 1.0), mean=0.011551131278114095, variance=0.006922456797977055, skewness=10.16078240925566, kurtosis=108.34485258005037)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(y_pred_log[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0,max_depth=8)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_rdtree = classifier.predict_proba(X_vali)\n",
    "result(y_vali,y_pred_rdtree[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_proba(X_vali)\n",
    "result(y_vali,y_pred_rdtree[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "y_rfc = rfc.predict_proba(X_vali)\n",
    "result(y_vali,y_rfc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP值为： 0.8821342987982309\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Fitting XGBoost to the Training set\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "                            eta = 0.1,\n",
    "                            gamma = 0.05)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_xg = classifier.predict_proba(X_vali)\n",
    "\n",
    "result(y_vali,y_pred_xg[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=74416, minmax=(2.6233061e-05, 0.9986639), mean=0.010523386, variance=0.0075099496, skewness=10.009506225585938, kurtosis=103.08457975240842)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(y_pred_xg[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, \n",
    "                                      monotone_constraints=None,\n",
    "                                     n_estimators=1000,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None,\n",
    "             eta =0.05)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_xg = classifier.predict_proba(X_vali)\n",
    "\n",
    "result(y_vali,y_pred_xg[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboosting 做模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {'objective':['binary:logistic'],\n",
    "              'eta': [0.05,0.1],\n",
    "              'gamma':[0.00,0.05]\n",
    "              \n",
    "#               'eta': [0.05,0.1,0.25,0.3],\n",
    "#               'max_depth': [5,10,15,20],\n",
    "#               'subsample': [0.5,0.6],\n",
    "#               'gamma':[0.00,0.05,0.10,0.15],\n",
    "#               'scale_pos_weight': [1, 30],\n",
    "#               'min_child_weight': [1,3],\n",
    "#               'n_estimators': [100,200,500,1000]\n",
    "             }\n",
    "\n",
    "# {'objective':['binary:logistic'],\n",
    "#               'eta': np.arange(0.01,0.4,0.05),\n",
    "#               'max_depth': np.arange(5,35,5),\n",
    "#               'subsample': [0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "#               'colsample_bytree': [0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "#               'gamma':[0.00,0.05,0.10,0.15,0.20],\n",
    "#               'scale_pos_weight': [1, 30,40,50,300,400,500,600,700],\n",
    "#               'min_child_weight': [1,3,6,8,11],\n",
    "#               'n_estimators': np.arange(100,1100,120)}\n",
    "\n",
    "# parameters = {'objective':['binary:logistic'],\n",
    "#               'eta': [0.01,0.02],\n",
    "#               'max_depth': [10,15],\n",
    "#               'subsample': [0.5,0.6],\n",
    "#               'colsample_bytree': [0.5,0.6],\n",
    "#               'gamma':[0.00,0.05],\n",
    "#               'scale_pos_weight': [1, 30],\n",
    "#               'min_child_weight': [1,3],\n",
    "#               'n_estimators': [100]}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, \n",
    "                   parameters, \n",
    "                   n_jobs=5, \n",
    "                   cv = StratifiedShuffleSplit(n_splits=5,test_size=0.25,random_state = 0),\n",
    "                   scoring='average_precision',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([pd.DataFrame(clf.cv_results_[\"params\"]),pd.DataFrame(clf.cv_results_[\"mean_test_score\"], columns=[\"average_precision\"])],axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数字典\n",
    "param_grid = {'learning_rate':[0.05,0.1,0.25,0.3],\n",
    "              'max_depth':range(2,10),\n",
    "              'n_estimators':range(100,1100,120)}\n",
    "kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set = [(x_test,y_test)],          # 评估数据集\n",
    "          eval_metric = \"mlogloss\",              # 评估标准\n",
    "          early_stopping_rounds = 10,            # 当loss有10次未变，提前结束评估\n",
    "          verbose = False)                       # 显示提前结束\n",
    "\n",
    "\n",
    "# 参数字典\n",
    "param_grid = {'learning_rate':[0.05,0.1,0.25,0.3],\n",
    "              'max_depth':range(2,10),\n",
    "              'n_estimators':range(100,110,120)}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "grid_search = GridSearchCV(model,                   # 模型\n",
    "                           param_grid,              # 待调参数（字典）\n",
    "                           scoring=\"neg_log_loss\",  # 模型评估准则\n",
    "                           n_jobs=1,               # -1表示使用全部的cpu运算\n",
    "                           cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "def ceate_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    i = 0\n",
    "    for feat in features:\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "        i = i + 1\n",
    "\n",
    "    outfile.close()\n",
    "\n",
    "# def get_data():\n",
    "#     train = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "#     features = list(train.columns[2:])\n",
    "\n",
    "#     y_train = train.Hazard\n",
    "\n",
    "#     for feat in train.select_dtypes(include=['object']).columns:\n",
    "#         m = train.groupby([feat])['Hazard'].mean()\n",
    "#         train[feat].replace(m,inplace=True)\n",
    "\n",
    "#     x_train = train[features]\n",
    "\n",
    "#     return features, x_train, y_train\n",
    "\n",
    "\n",
    "# features, x_train, y_train = get_data()\n",
    "# ceate_feature_map(features)\n",
    "\n",
    "xgb_params = {\"objective\": \"reg:linear\", \"eta\": 0.01, \"max_depth\": 8, \"seed\": 42}\n",
    "num_rounds = 1000\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "importance = gbdt.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "# plt.figure()\n",
    "# df.plot()\n",
    "# df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "# plt.title('XGBoost Feature Importance')\n",
    "# plt.xlabel('relative importance')\n",
    "# plt.gcf().savefig('feature_importance_xgb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to be tuned\n",
    "tune_dic = {}\n",
    "tune_dic['max_depth']= [5,10,15,20,25] ## 树的最大深度\n",
    "tune_dic['eta']= [0.01,0.025,0.05,0.10,0.20]  ## learning rate\n",
    "tune_dic['gamma']= [0.00,0.05,0.10,0.15,0.20]  ## minimum loss function reduction required for a split\n",
    "\n",
    "res = {'result':[[[0,0,0],0]]}\n",
    "res_ary =np.array([])\n",
    "\n",
    "for depth in tune_dic['max_depth']:\n",
    "\n",
    "    for eta in tune_dic['eta']:\n",
    "        \n",
    "        for gamma in tune_dic['gamma']:\n",
    "\n",
    "            print(\"depth:\",depth, \"eta:\",eta,\"gamma:\",gamma, \"training starts\")\n",
    "            classifier = XGBClassifier(max_depth = depth,\n",
    "                                      eta = eta,\n",
    "                                      gamma = gamma\n",
    "                                      )\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict_proba(X_vali)\n",
    "            ap = average_precision_score(y_vali, y_pred[:,1])\n",
    "            \n",
    "            res_ary = np.append(res_ary,ap)\n",
    "            res[\"result\"].append([[depth,eta,gamma],ap])\n",
    "            \n",
    "            print(\"depth:\",depth, \"eta:\",eta,\"gamma:\",gamma, \"test finish\", \"ap value\",ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(res_ary)\n",
    "#depth: 5 eta: 0.1 gamma: 0.2 test finish ap value 0.8982911924028751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "def ANN_model(X_train,y_train,X_test,Batch_size=500, Epochs = 20):\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "    # 建立模型\n",
    "    classifier.add(Dense(units = 14,  activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 8, activation = 'relu'))\n",
    "    classifier.add(Dense(units = 4, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.3))\n",
    "    #输出层\n",
    "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # 将模型运用到训练集中\n",
    "    classifier.fit(X_train, y_train, batch_size = 500, epochs=Epochs)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"预测完成,返回预测值\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ann = ANN_model(X_train,y_train,X_vali,Epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result(y_vali,(y_pred_ann.reshape(y_pred_ann.shape[0],)+y_pred_xg[:,1])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习模型和xgboost都表现的很好， 结合两个模型结果更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result(y_vali,(y_pred_ann.reshape(y_pred_ann.shape[0],)+y_pred_xg[:,1])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_m_train_vali = main_df.filter(f.col(\"kind\")==\"train\").withColumn(\"用户平均消费金额\",avg(\"amount\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    ".withColumn(\"产品售出总数\",count(\"merchant_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "    .withColumn(\"产品客户数量\",f.approx_count_distinct(\"customer_id\").over(Window.partitionBy(\"merchant_id\")))\\\n",
    "    .withColumn(\"平均购买数量\",f.col(\"产品售出总数\")/f.col(\"产品客户数量\"))\\\n",
    "    .select(\"merchant_id\",\"用户平均消费金额\",\"平均购买数量\")\\\n",
    "    .distinct() \n",
    "\n",
    "\n",
    "df_trainVali = main_df.filter(f.col(\"kind\")==\"train\").join(c_m_train_vali,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\")\n",
    "\n",
    "df_test = main_df.filter(f.col(\"kind\")==\"test\").join(c_m_train_vali,\"merchant_id\",\"left\").drop(\"customer_id\",\"merchant_id\")\n",
    "\n",
    "test_id = np.array(df_test.select(\"id\").collect(),dtype=\"float\").astype(np.float32)\n",
    "\n",
    "X_train_m = process_data(df_trainVali)\n",
    "y_train_m = np.array(df_trainVali.select(\"fraud\").collect(),dtype=\"float\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Xgboost\n",
    "classifier = XGBClassifier(max_depth = 5,\n",
    "                            eta = 0.1,\n",
    "                            gamma = 0.2)\n",
    "classifier.fit(X_train_m, y_train_m)\n",
    "y_xg_final = classifier.predict_proba(X_test)[:,1]\n",
    "print(\"Xgboost预测完成\")\n",
    "\n",
    "y_ann_final = ANN_model(X_train_m,y_train_m,X_test,Epochs = 200)\n",
    "print(\"ANN预测完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_ary = (y_xg_final + y_ann_final.reshape(y_ann_final.shape[0],))/2\n",
    "test_result = np.concatenate((test_id,np.array(pred_ary).reshape(pred_ary.shape[0],1)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出结果 存为csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_result,columns = [\"id\",\"预测为1的可能性\"]).to_csv(\"update_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督学习 Autoencoder： 效果非常好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        first_layer,second_layer,third_layer,middle_layer = kwargs[\"layers\"]\n",
    "\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=first_layer, out_features=second_layer)\n",
    "    #     self.doe1 = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.enc2 = nn.Linear(in_features=second_layer, out_features=third_layer)\n",
    "    #     self.doe2 = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.enc3 = nn.Linear(in_features=third_layer, out_features=middle_layer)\n",
    "    #     self.doe3 = nn.Dropout(p=0.5, inplace=False)\n",
    "\n",
    "        # mid\n",
    "        self.mid = nn.Linear(in_features=middle_layer, out_features=middle_layer)  \n",
    "\n",
    "        # decoder \n",
    "    #     self.dod3 = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.dec3 = nn.Linear(in_features=middle_layer, out_features=third_layer)\n",
    "    #     self.dod2 = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.dec2 = nn.Linear(in_features=third_layer, out_features=second_layer)\n",
    "    #     self.dod1 = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.dec1 = nn.Linear(in_features=second_layer, out_features=first_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.enc1(x))\n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.enc2(x))\n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.enc3(x))\n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.mid(x))\n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.dec3(x))\n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.dec2(x))             \n",
    "        #     x = nn.Dropout(p=0.5, inplace=False)(x)\n",
    "        x = F.relu(self.dec1(x))\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_Train(X_train, model_layers, BATCH_SIZE , NUM_EPOCHS = 100,LEARNING_RATE= 0.0002):\n",
    "\n",
    "    manualSeed = 888\n",
    "    #manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "    print(\"Random Seed: \", manualSeed)\n",
    "    torch.manual_seed(manualSeed)    \n",
    "\n",
    "    model = Autoencoder(layers = model_layers)\n",
    "    print(model)\n",
    "\n",
    "    print(\"Reset model parameters\")\n",
    "    for name, module in model.named_children():\n",
    "        module.reset_parameters()\n",
    "\n",
    "    ngpu = 4\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.to(device)\n",
    "\n",
    "    trainloader = DataLoader(X_train, batch_size=BATCH_SIZE, shuffle=True) ##number of batches \n",
    "    testloader = DataLoader(X_train, batch_size=1, shuffle=False)\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss= []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            batch = data\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            print('Epoch {} of {}, Train Loss: {:.3f}'.format(epoch+1, NUM_EPOCHS, loss))\n",
    "\n",
    "    print(\"test starts\")\n",
    "\n",
    "    for idx,data in enumerate(testloader):\n",
    "        batch = data.to(device)\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        test_loss.append(loss)\n",
    "\n",
    "    print(\"test finished\")\n",
    "\n",
    "    return train_loss,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss,test_loss = AE_Train(X_train_m, [30,20,10,4], 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算模型输出值于输入值的MSE, MSE越大，说明越有可能是fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_li = [int(row.id) for row in df_trainVali.select(\"id\").collect()]\n",
    "los = [float(i) for i in test_loss]\n",
    "\n",
    "df_loss = spark.createDataFrame(zip(id_li,los),[\"id\",\"loss\"])\n",
    "# df_loss.show()\n",
    "\n",
    "result_df = df_trainVali.join(df_loss,\"id\",\"left\").select(\"id\",\"fraud\",\"loss\")\n",
    "\n",
    "result_df.orderBy(desc(\"loss\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.filter(f.col('fraud') == 1).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.filter(f.col('fraud') == 0).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面这张图 表示通过autoencoder模型算出来的loss（MSE)值大于15（左右），结果100%会是fraud 当loss值在3～15之间时，结果有可能是fraud 也可能不是fraud, 但整体而言这个预测的非常好，下一步可以做的是去掉loss值大的数据，重新训练autoencoder模型，也可以适当增多feature, 再进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "ls = [float(i) for i in test_loss]\n",
    "plt.scatter(ls,y_train_m);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
